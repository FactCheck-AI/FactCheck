A character type.

The char type represents a single character. More specifically, since âcharacterâ isnât a well-defined concept in Unicode, char is a âUnicode scalar valueâ.

This documentation describes a number of methods and trait implementations on the char type. For technical reasons, there is additional, separate documentation in the std::char module as well.

Â§Validity and Layout

A char is a âUnicode scalar valueâ, which is any âUnicode code pointâ other than a surrogate code point. This has a fixed numerical definition: code points are in the range 0 to 0x10FFFF, inclusive. Surrogate code points, used by UTF-16, are in the range 0xD800 to 0xDFFF.

No char may be constructed, whether as a literal or at runtime, that is not a Unicode scalar value. Violating this rule causes undefined behavior.

Unicode scalar values are also the exact set of values that may be encoded in UTF-8. Because char values are Unicode scalar values and functions may assume incoming str values are valid UTF-8, it is safe to store any char in a str or read any character from a str as a char.

The gap in valid char values is understood by the compiler, so in the below example the two ranges are understood to cover the whole range of possible char values and there is no error for a non-exhaustive match.

All Unicode scalar values are valid char values, but not all of them represent a real character. Many Unicode scalar values are not currently assigned to a character, but may be in the future (âreservedâ); some will never be a character (ânoncharactersâ); and some may be given different meanings by different users (âprivate useâ).

char is guaranteed to have the same size, alignment, and function call ABI as u32 on all platforms.

Â§Representation

char is always four bytes in size. This is a different representation than a given character would have as part of a String. For example:

As always, remember that a human intuition for âcharacterâ might not map to Unicodeâs definitions. For example, despite looking similar, the âÃ©â character is one Unicode code point while âeÌâ is two Unicode code points:

This means that the contents of the first string above will fit into a char while the contents of the second string will not. Trying to create a char literal with the contents of the second string gives an error:

error: character literal may only contain one codepoint: 'eÌ' let c = 'eÌ'; ^^^